{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identifying Fraud from Enron Emails\n",
    "\n",
    "Marcus Kehn\n",
    "\n",
    "\n",
    "\n",
    "## Introduction\n",
    "> Summarize for us the goal of this project and how machine learning is useful in trying to accomplish it. As part of your answer, give some background on the dataset and how it can be used to answer the project question.\n",
    "\n",
    "\n",
    "Some of you reading this might not know of Enron, at one time it was one of the largest companies in the world. Quickly thereafter, wide spread corporate fraud at Enron caused it to collapse into bankruptcy. What this project deals with is the aftermath of the rise and fall of a giant torn apart from the inside. We have been given access to what is rather rare, a massive dataset with publicly accessible personally identifying information which is legal to obtain. This comes as a result of a Federal court case, against Enron.\n",
    "\n",
    "The dataset taken from these court proceedings, has been appended by a list of persons of interests in the previously mentioned federal court case. For further clarification, persons of interest (POIs) are people who were charged and found guilty of crimes, plead guild for immunity, or reached a settlement.\n",
    "\n",
    "\n",
    "The combined data set has: \n",
    "* 146 rows\n",
    "* 14 financial \n",
    "* 6 email features\n",
    "* one labeled feature (POI)\n",
    "\n",
    "\n",
    "\n",
    "This project is to create a model, that when using the ideal features, can correctly decide if a person is a POI.\n",
    "With such rare data like legally obtainable personal identifying data in bulk, it has been used many times to help identify and reduce fraud."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration & Outlier Investigation\n",
    "> Were there any outliers in the data when you got it, and how did you handle those? [relevant rubric items: “data exploration”, “outlier investigation”]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "First, we convert the dict to a dataframe as it's easier to work with."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "df = pd.DataFrame.from_records(data_dict).T\n",
    "print(df.shape)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "First thing I notice is NaN values and numeric data in the same column. This tells me that the data is not stored as a float, or an int value.\n",
    "This means we need a bit of data cleaning, and the first step to that is making sure we focus on only the numeric columns, as NaN values in the str email is acceptable, for now."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "num_cols = [i for i in df.columns if i != 'email_address']\n",
    "df[num_cols] = df[num_cols].astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Here we iterate through the column names and add each column name, that isn't email_address as it's the only string column, into a list.\n",
    "After which, we cast the numeric columns to float using the pandas.Dataframe.astype(float) which is inherited by our Variable df which is a pandas.Dataframe.\n",
    "\n",
    "Proceeding, removing the NaN string values from email_address columns, as numpy.nan so we can change them later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df.loc[df.email_address == 'NaN', 'email_address'] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the exploratory data phase, I was able to weed out three obvious outliers:\n",
    "\n",
    "* TOTAL: The sum of all the other records, much too large.\n",
    "* THE TRAVEL AGENCY IN THE PARK: The travel agency in the park cannot be an employee, thus we remove them.\n",
    "* LOCKHART EUGENE E: This record contains only null values, we remove for more accurate representation of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df = df[(df.index != 'TOTAL') & (df.index != 'THE TRAVEL AGENCY IN THE PARK') & (df.index != 'LOCKHART EUGENE E')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Next, we move on to feature selection\n",
    "\n",
    "\n",
    "## Feature Selection\n",
    ">What features did you end up using in your POI identifier, and what selection process did you use to pick them? Did you have to do any scaling? Why or why not? As part of the assignment, you should attempt to engineer your own feature that does not come ready-made in the dataset -- explain what feature you tried to make, and the rationale behind it. (You do not necessarily have to use it in the final analysis, only engineer and test it.) In your feature selection step, if you used an algorithm like a decision tree, please also give the feature importances of the features that you use, and if you used an automated feature selection function like SelectKBest, please report the feature scores and reasons for your choice of parameter values. [relevant rubric items: “create new features”, “intelligently select features”, “properly scale features”]\n",
    "\n",
    "\n",
    "We've developed two extra features: exercised_expense_ratio, which is simply the ratio of exercised_stocks_options to expenses, and bonus_expense_ration, which is used to\n",
    "determine the ratio of the bonus divided by the expenses of the same record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "### We create a new feature\n",
    "df['exercised_expense_ratio'] = df.exercised_stock_options / df.expenses\n",
    "df['bonus_expense_ratio'] = df.bonus / df.expenses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Instead of iterating over every feature included by default, we chose 3 of the  14 financial features at a time and put them into our list to use. This process was done manually. From there used RFECV to verify they were all of equal importance and to autoremove any that weren't going forwards."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "features_list2 = ['exercised_expense_ratio', 'bonus_expense_ratio']\n",
    "features_list = features_list + features_list2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "rkf = RepeatedKFold(n_splits=5, n_repeats=10, random_state=2652124)\n",
    "rfecv = RFECV(estimator=best_random_tree, step=1, cv=rkf,\n",
    "              min_features_to_select=min_features_to_select, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Print the ranking of features that rfe suggest we use, with 1 representing keep, and 2+ representing what is less impactful in order of impact\n",
    "print(rfecv.ranking_)\n",
    "\n",
    "###[1 1 1 1 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Print the ideal number of features suggested by the algorithm\n",
    "print(\"Ideal number of features : %d\" % rfecv.n_features_)\n",
    "\n",
    "###Ideal number of features : 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img height=\"800\" src=\"graph.png\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Meaning, the final precision and recall values we will obtain, were based on the programmatical selection of features utilizing RFECV."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Algorithm Selection\n",
    "\n",
    ">What algorithm did you end up using? What other one(s) did you try? How did model performance differ between algorithms? [relevant rubric item: “pick an algorithm”]\n",
    "\n",
    "We go ahead and test two different algorithms, DecisionTreeClassifier and RandomTreeClassifier, I went with these for two reasons:\n",
    "    1. Experience\n",
    "    2. No need to scale features\n",
    "\n",
    "The ability to build, deploy and realize an ML algorithm is very important, and so I chose something I could rock with quickly.\n",
    "We also combine this with the power of RFECV to impliment recursive feature selection with cross validation.\n",
    "This gives us just a bit more confidence in our algorithm's results.\n",
    "\n",
    "We eventually ended up going with an algorithm combination of RandomTreeClassifier, RFECV, and used RepeatedKFold as our cross-validator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "rkf = RepeatedKFold(n_splits=5, n_repeats=10, random_state=2652124)\n",
    "rfecv = RFECV(estimator=best_random_tree, step=1, cv=rkf,\n",
    "              min_features_to_select=min_features_to_select, n_jobs=-1)\n",
    "rfecv.fit(features_train, labels_train)\n",
    "y = rfecv.predict(features_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Parameter and Algorithm Tuning\n",
    "> What does it mean to tune the parameters of an algorithm, and what can happen if you don’t do this well? How did you tune the parameters of your particular algorithm? What parameters did you tune? (Some algorithms do not have parameters that you need to tune -- if this is the case for the one you picked, identify and briefly explain how you would have done it for the model that was not your final choice or a different model that does utilize parameter tuning, e.g. a decision tree classifier). [relevant rubric items: “discuss parameter tuning”, “tune the algorithm”]\n",
    "\n",
    "Hyper-parameters are parameters we can utilize to change the chosen algorithm slightly. By tuning them we try are able to find the optimal setting for maximum f1 score.\n",
    "\n",
    "Algo tuning needs caution because overtuning the hyper-parameters may result overfitting and undertuning can lead to underfitting it.\n",
    "\n",
    "I used GridSearchCV to calculate the best possible parameters for the DecisionTreeClassifier from the following grid of possible choices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "rclf = RandomForestClassifier()\n",
    "\n",
    "param_grid = {\n",
    "    \"n_estimators\": [9, 18, 27, 36],\n",
    "    \"max_depth\": [None, 1, 5, 10, 15],\n",
    "    \"min_samples_leaf\": [1, 2, 4, 6]}\n",
    "\n",
    "# Use GridSearchCV to find the optimal hyperparameters for the classifier\n",
    "rclf_grid = GridSearchCV(rclf, param_grid=param_grid, scoring='f1', cv=5)\n",
    "rclf_grid.fit(features, labels)\n",
    "# Get the best algorithm hyperparameters for the Decision Tree\n",
    "print(rclf_grid.best_params_)\n",
    "best_random_tree = clf_grid.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Validation\n",
    "\n",
    "We touched on this briefly before, but we utilise cross-validation using RFECV equipped with RepeatedKFold as the\n",
    "cross validation algorithm.\n",
    "\n",
    "We don't want to underfit or over fit, but we also want a consistent result from a \"Random\" classifier\n",
    "\n",
    "For consitency sake and testing sakes therefore we utilize the random_state param of RepeatedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "rkf = RepeatedKFold(n_splits=5, n_repeats=10, random_state=13373003135)\n",
    "rfecv = RFECV(estimator=best_random_tree, step=1, cv=rkf,\n",
    "              min_features_to_select=min_features_to_select, n_jobs=-1)\n",
    "rfecv.fit(features_train, labels_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Evaluation Metrics\n",
    "> Give at least 2 evaluation metrics and your average performance for each of them. Explain an interpretation of your metrics that says something human-understandable about your algorithm’s performance. [relevant rubric item: “usage of evaluation metrics”]\n",
    "\n",
    "Precision, Recall and F1 are useful for determining the success of our prediction.\n",
    "\n",
    "    Precision measures: Of all the sample records we classified as true how many are actually true?\n",
    "    Recall measures: Of all the actually true sample records, how many did we classify as true?\n",
    "\n",
    "F1 is the weighted average of the precision and recall, with the score ideal at 1 and least ideal at 0.\n",
    "\n",
    "- Our precision score of 0.44904 means that ~44.9% of the individuals labeled as POI were actually POI.\n",
    "\n",
    "- Our recall score of 0.30400 means ~30.4% of POI in the dataset were identified correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "Accuracy: 0.84729\tPrecision: 0.44904\tRecall: 0.30400\tF1: 0.36255\tF2: 0.32499\n",
    "\tTotal predictions: 14000\tTrue positives:  608\tFalse positives:  746\tFalse negatives: 1392\tTrue negatives: 11254"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Sources:\n",
    "\n",
    "http://scikit-learn.org/stable/modules/feature_selection.html\n",
    "\n",
    "http://scikit-learn.org/stable/modules/model_evaluation.html\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\n",
    "\n",
    "http://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RepeatedKFold.html\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}